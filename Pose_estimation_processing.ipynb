{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install openpyxl\n",
    "!pip3 install weasyprint\n",
    "!pip3 install gizeh\n",
    "!pip3 install pycairo\n",
    "!pip3 install cairocffi\n",
    "!pip3 install more_itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter online\n"
     ]
    }
   ],
   "source": [
    "import gizeh as gz\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from more_itertools.recipes import grouper, pairwise\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "print(\"Jupyter online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#Openpose keypointdata extractor\n",
    "\n",
    "#https://sduxbury.medium.com/how-you-can-build-practical-applications-by-quantifying-observations-from-video-e266b945eea0\n",
    "#Point sets are in the form [coordinate, coordinate, confidence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(jsonfile):\n",
    "    jsonopen = open(jsonfile)\n",
    "    jfile = json.load(jsonopen)\n",
    "    jsonopen.close()\n",
    "    x = pd.DataFrame(jfile)\n",
    "    return x\n",
    "\n",
    "def framenames(path, fps):\n",
    "    frames = np.arange(0, 900*fps+1, 24).tolist()\n",
    "    startname = 'frame'\n",
    "    endname = '_keypoints.json'\n",
    "    frames = frames[1:]\n",
    "    framelist = []\n",
    "    for i in frames:\n",
    "        framelist.append(path+startname+str(i)+endname)\n",
    "    return(framelist)\n",
    "\n",
    "def data_loader(framelist):\n",
    "    pose1= []\n",
    "    pose2= []\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "    for jsonfile in framelist:\n",
    "        data = open_json(jsonfile)\n",
    "        if len(data.index) == 2:\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[1,:]\n",
    "            count1 += 1\n",
    "        if len(data.index) == 3:\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[2,:]\n",
    "            count2 += 1\n",
    "        if len(data.index) == 1:\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[1,:]\n",
    "            count3 += 1\n",
    "        if len(data.index) == 4:        #NEEDS WORK, KN5 frame 216 has 4 skeletons in it\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[1,:]\n",
    "            count4 += 1\n",
    "        if len(data.index) == 5:\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[1,:]\n",
    "            count5 += 1\n",
    "\n",
    "        sig1 = s1[\"people\"]\n",
    "        sig2 = s2[\"people\"]\n",
    "\n",
    "        signer1poses = pd.DataFrame.from_dict(sig1, orient='index')\n",
    "        signer2poses = pd.DataFrame.from_dict(sig2, orient='index')\n",
    "\n",
    "        signer1 = signer1poses.iloc[1:5,:]\n",
    "        signer2 = signer2poses.iloc[1:5,:]\n",
    "\n",
    "        signer1 = signer1.drop([\"face_keypoints_2d\"])\n",
    "        signer2 = signer2.drop([\"face_keypoints_2d\"])\n",
    "        \n",
    "        pose1.append(signer1)\n",
    "        pose2.append(signer2)\n",
    "    poses = pose1 + pose2\n",
    "    print('There are '+str(count3)+' frames with 1 person in them')\n",
    "    print('There are '+str(count1)+' frames with 2 persons in them')\n",
    "    print('There are '+str(count2)+' frames with 3 persons in them')\n",
    "    print('There are '+str(count4)+' frames with 4 persons in them')\n",
    "    print('There are '+str(count5)+' frames with 5 persons in them')\n",
    "    return poses\n",
    "\n",
    "def scale(lst):\n",
    "    newdata = []\n",
    "    for i,d in enumerate(lst):\n",
    "        if d.any() > 0:\n",
    "            newdata.append(d/500)\n",
    "        else:\n",
    "            newdata.append(d)\n",
    "    return newdata\n",
    "\n",
    "def normalizer(lst,nmbr):\n",
    "    fin = []\n",
    "    norm = (nmbr*1.8)\n",
    "    #assuming person 2 always sits on the right in the frame which gives him/her different y coordinates but similar x coordinates.\n",
    "    z = np.array([norm,0])\n",
    "    #normalize the y coordinate appropriately for person 1 so relative distances stay the same. \n",
    "    for i,d in enumerate(lst):\n",
    "        if d.any() > 0:\n",
    "            d = d-z\n",
    "            fin.append(d)\n",
    "        else:\n",
    "            fin.append(d) \n",
    "    return fin\n",
    "\n",
    "def pose_processor(poses):                                                      #Finish pose processor\n",
    "    poses = poses\n",
    "    lengthp = len(poses)\n",
    "    middle = lengthp/2\n",
    "    p1 = poses[:int(middle)]\n",
    "    p2 = poses[int(middle):]\n",
    "\n",
    "    #Create confidenceintervals index\n",
    "    confidenceintervals = np.arange(-1, 75, 3).tolist()\n",
    "    confidenceintervals = confidenceintervals[1:]\n",
    "    #print(confidenceintervals)\n",
    "\n",
    "    #Create x and y coordinates index\n",
    "    coordinates = np.arange(0, 75, 1).tolist()\n",
    "    cs1 = set(confidenceintervals)\n",
    "    coorindex = [x for x in coordinates if x not in cs1]\n",
    "    #print(coorindex)\n",
    "\n",
    "    person1 = []\n",
    "    for framepose in p1:\n",
    "        signer1 = pd.DataFrame(framepose) \n",
    "        skel1 = signer1.iloc[0,:]\n",
    "        #Get confidence intervals for signer 1 and 2 per keypoint\n",
    "        ci1 = [skel1[i] for i in confidenceintervals]\n",
    "        #Get x and y coordinates for signer 1 and 2 per keypoint\n",
    "        skelcoor1 = [skel1[i] for i in coorindex]\n",
    "        sk1 = np.array_split(skelcoor1, 25)\n",
    "        #Normalize on neck Y coordinate as neck position\n",
    "        neck2 = sk1[1]\n",
    "        newdata1 = scale(normalizer(sk1, neck2[0]))\n",
    "        person1.append(newdata1)\n",
    "        \n",
    "    person2 = []\n",
    "    for framepose in p2:\n",
    "        signer2 = pd.DataFrame(framepose) \n",
    "        skel2 = signer2.iloc[0,:]\n",
    "        #Get confidence intervals for signer 1 and 2 per keypoint\n",
    "        ci2 = [skel2[i] for i in confidenceintervals]\n",
    "        #Get x and y coordinates for signer 1 and 2 per keypoint\n",
    "        skelcoor2 = [skel2[i] for i in coorindex]\n",
    "        sk2 = np.array_split(skelcoor2, 25)\n",
    "\n",
    "        newdata2 = scale(sk2)\n",
    "        person2.append(newdata2)\n",
    "    #return sk1, sk2\n",
    "    signers = person1 + person2\n",
    "    return signers\n",
    "\n",
    "def hand_processor(poses):\n",
    "    poses = poses\n",
    "    lengthp = len(poses)\n",
    "    middle = lengthp/2\n",
    "    p1 = poses[:int(middle)]\n",
    "    p2 = poses[int(middle):]\n",
    "\n",
    "    #Create confidenceintervals index\n",
    "    confidenceintervalh = np.arange(-1, 63, 3).tolist()\n",
    "    confidenceintervalh = confidenceintervalh[1:]\n",
    "    #print(confidenceintervalh)\n",
    "\n",
    "    #Create x and y coordinates index\n",
    "    coordinateh = np.arange(0, 63, 1).tolist()\n",
    "    csi2 = set(confidenceintervalh)\n",
    "    coorindexhand = [x for x in coordinateh if x not in csi2]\n",
    "    #print(coorindexhand)\n",
    "\n",
    "    person1 = []                                        #CHANGE VARIABLE NAME\n",
    "    for framepose in p1:\n",
    "        signer1 = pd.DataFrame(framepose) \n",
    "        hand1_L = signer1.iloc[1,:]\n",
    "        hand1_R = signer1.iloc[2,:]\n",
    "        hand1_L = hand1_L.dropna(axis='rows')\n",
    "        hand1_R = hand1_R.dropna(axis='rows')\n",
    "        #Get confidence intervals for signer 1 and 2 per keypoint per hand\n",
    "        H1L = [hand1_L[i] for i in confidenceintervalh]\n",
    "        H1R = [hand1_R[i] for i in confidenceintervalh]\n",
    "        #Get x and y coordinates for signer 1 and 2 per keypoint\n",
    "        hand1R = [hand1_L[i] for i in coorindexhand]\n",
    "        hand1L = [hand1_R[i] for i in coorindexhand]\n",
    "        #print(len(hand1L))\n",
    "        #Split the list into x, y coordinate arrays\n",
    "        handR11 = np.array_split(hand1R, 21)\n",
    "        handL11 = np.array_split(hand1L, 21)\n",
    "        #Normalize on 240, from Songha Ban's code.\n",
    "        handR1 = scale(normalizer(handR11, 240))\n",
    "        handL1 = scale(normalizer(handL11, 240))\n",
    "        #Join them into 1 list with numpy arrays\n",
    "        hand1 = [handR1, handL1]\n",
    "        person1.append(hand1)\n",
    "\n",
    "    person2 = []                                        #CHANGE VARIABLE NAME\n",
    "    for framepose in p2:\n",
    "        signer2 = pd.DataFrame(framepose) \n",
    "        hand2_L = signer2.iloc[1,:]\n",
    "        hand2_R = signer2.iloc[2,:]\n",
    "        hand2_L = hand2_L.dropna(axis='rows')\n",
    "        hand2_R = hand2_R.dropna(axis='rows')\n",
    "        #Get confidence intervals for signer 1 and 2 per keypoint per hand\n",
    "        H2L = [hand2_R[i] for i in confidenceintervalh]\n",
    "        H2R = [hand2_R[i] for i in confidenceintervalh]\n",
    "        #Get x and y coordinates for signer 1 and 2 per keypoint\n",
    "        hand2R = [hand2_L[i] for i in coorindexhand]\n",
    "        hand2L = [hand2_R[i] for i in coorindexhand]\n",
    "        #print(len(hand1L))\n",
    "        #Split the list into x, y coordinate arrays\n",
    "        handR22 = np.array_split(hand2R, 21)\n",
    "        handL22 = np.array_split(hand2L, 21)\n",
    "        handR2 = scale(handR22)\n",
    "        handL2 = scale(handL22)\n",
    "        #Join them into 1 list with numpy arrays\n",
    "        hand2 = [handR2, handL2]\n",
    "        person2.append(hand2)\n",
    "    return person1, person2\n",
    "\n",
    "def data_shaper(signers, hands):\n",
    "    \n",
    "\n",
    "def label_loader(labelspath):\n",
    "    data = pd.read_csv(labelspath)\n",
    "    data2 = data.iloc[:,3]\n",
    "    labels = data2\n",
    "    return [labels]\n",
    "\n",
    "def finish_processing(path,fps,labelspath):\n",
    "    framelist = framenames(path, fps)\n",
    "    poses = data_loader(framelist)\n",
    "    processedsk = pose_processor(poses)\n",
    "    processedhands = hand_processor(poses)\n",
    "    features = data_shaper(processedsk,processedhands)\n",
    "    labels = label_loader(labelspath)\n",
    "    return processedhands\n",
    "\n",
    "#TO DO: COMBINE HANDS TO CORRECT SKELETON DATA AS FEATURES\n",
    "#       RESHAPE DATA TO CORRECT FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 frames with 1 person in them\n",
      "There are 809 frames with 2 persons in them\n",
      "There are 90 frames with 3 persons in them\n",
      "There are 1 frames with 4 persons in them\n",
      "There are 0 frames with 5 persons in them\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "path = 'D:\\Videos thesis\\KN5Jan7_poses\\KN5Jan7_poses\\\\'\n",
    "fps = 24\n",
    "labelspath = 'D:\\Videos thesis\\KN5jan7_annotated.csv'\n",
    "z = finish_processing(path, fps, labelspath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "895    1\n",
      "896    1\n",
      "897    3\n",
      "898    3\n",
      "899    3\n",
      "Name: class, Length: 900, dtype: int64]\n"
     ]
    }
   ],
   "source": [
    "labelspath = 'D:\\Videos thesis\\KN5jan7_annotated.csv'\n",
    "\n",
    "def label_loader(labelspath):\n",
    "    data = pd.read_csv(labelspath)\n",
    "    data2 = data.iloc[:,3]\n",
    "    labels = data2\n",
    "    return [labels]\n",
    "\n",
    "z = label_loader(labelspath)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT FOR LSTM INPUT\n",
    "# [samples, time steps and features]\n",
    "\n",
    "##                          ##\n",
    "##        MODEL             ##\n",
    "##                          ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spyjetson.blogspot.com/2019/10/jetsonnano-human-pose-estimation-using.html\n",
    "\n",
    "#SKELETON KEYPOINTS\n",
    "{\n",
    "//     {0,  \"Nose\"},\n",
    "//     {1,  \"Neck\"},\n",
    "//     {2,  \"RShoulder\"},\n",
    "//     {3,  \"RElbow\"},\n",
    "//     {4,  \"RWrist\"},\n",
    "//     {5,  \"LShoulder\"},\n",
    "//     {6,  \"LElbow\"},\n",
    "//     {7,  \"LWrist\"},\n",
    "//     {8,  \"MidHip\"},\n",
    "//     {9,  \"RHip\"},\n",
    "//     {10, \"RKnee\"},\n",
    "//     {11, \"RAnkle\"},\n",
    "//     {12, \"LHip\"},\n",
    "//     {13, \"LKnee\"},\n",
    "//     {14, \"LAnkle\"},\n",
    "//     {15, \"REye\"},\n",
    "//     {16, \"LEye\"},\n",
    "//     {17, \"REar\"},\n",
    "//     {18, \"LEar\"},\n",
    "//     {19, \"LBigToe\"},\n",
    "//     {20, \"LSmallToe\"},\n",
    "//     {21, \"LHeel\"},\n",
    "//     {22, \"RBigToe\"},\n",
    "//     {23, \"RSmallToe\"},\n",
    "//     {24, \"RHeel\"},\n",
    "//     {25, \"Background\"}\n",
    "// };\n",
    "\n",
    "#HAND KEYPOINTS\n",
    "#as seen on left hand when it is held open \n",
    "{\n",
    "//     {0,  \"BotR palm\"},\n",
    "//     {1,  \"BotL palm\"},\n",
    "//     {2,  \"Thumb Bot\"},\n",
    "//     {3,  \"Thumb Mid\"},\n",
    "//     {4,  \"Thumb Top\"},\n",
    "//     {5,  \"Index Bot\"},\n",
    "//     {6,  \"Index Mid1\"},\n",
    "//     {7,  \"Index Mid2\"},\n",
    "//     {8,  \"Index Top\"},\n",
    "//     {9,  \"Middle Bot\"},\n",
    "//     {10, \"Middle Mid1\"},\n",
    "//     {11, \"Middle Mid2\"},\n",
    "//     {12, \"Middle Top\"},\n",
    "//     {13, \"Ring Bot\"},\n",
    "//     {14, \"Ring Mid1\"},\n",
    "//     {15, \"Ring Mid2\"},\n",
    "//     {16, \"Ring Top\"},\n",
    "//     {17, \"Pinky Bot\"},\n",
    "//     {18, \"Pinky Mid1\"},\n",
    "//     {19, \"Pinky Mid2\"},\n",
    "//     {20, \"Pinky Top\"},\n",
    "// };\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeleton12.to_excel(\"signer2.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "#Openpose Skeleton\n",
    "#https://cmu-perceptual-computing-lab.github.io/openpose/web/html/doc/md_doc_02_output.html\n",
    "#https://github.com/SonghaBan/DancingAI/blob/main/visframe.py \n",
    "\n",
    "l_pair_openpose25 = [\n",
    "    [1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], \n",
    "    [8, 9], [9, 10], [10,11], [11,24], [11,22], [22,23],\n",
    "    [8,12], [12, 13], [13,14], [14,21], [14,19], [19,20],\n",
    "    [1, 0], \n",
    "    [0, 16], [18, 16], \n",
    "    [0, 15], [15, 17],\n",
    "    ]\n",
    "\n",
    "colors_openpose25 = np.array([\n",
    "    [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0],\n",
    "    [85, 255, 0], [255,0,0], \n",
    "    [0, 255, 0], [0, 255, 85], [0, 255, 85], [0, 255, 85], [0, 255, 85], [0, 255, 85], \n",
    "    [0, 255, 170], [0, 255, 255],[0, 170, 255],[0, 170, 255],[0, 170, 255],[0, 170, 255],\n",
    "    [0, 85, 255], \n",
    "#    [0, 0, 255], [85, 0, 255], \n",
    "#    [170, 0, 255],\n",
    "    [255, 0, 255], [255, 0, 170], \n",
    "    [255, 0, 85], [255, 0, 0]])/255\n",
    "\n",
    "print(len(l_pair_openpose25))\n",
    "print(len(colors_openpose25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Frame\n",
    "#https://github.com/SonghaBan/DancingAI/blob/main/visframe.py \n",
    "\n",
    "#import torch\n",
    "#import math\n",
    "#import cv2\n",
    "#import time\n",
    "#from more_itertools.recipes import grouper, pairwise\n",
    "#import moviepy.editor as mpy\n",
    "#import more_itertools\n",
    "\n",
    "\n",
    "\n",
    "def show_img(arr,idx=0):\n",
    "    img = Image.fromarray(arr)\n",
    "#    img.show()\n",
    "    img.save('test1.jpg')\n",
    "\n",
    "def visualize_frame(frame, joints=17, label=0):\n",
    "    \n",
    "    surface = gz.Surface(width=640, height=360, bg_color=(1,1,1))\n",
    "    \n",
    "    frame = open(frame)\n",
    "    frame = json.load(frame)\n",
    "    #frame.close()\n",
    "\n",
    "    if joints == 25:\n",
    "        frame = frame['people'][0]\n",
    "        n_group = int(len(frame['pose_keypoints_2d']) / joints)\n",
    "        pose = list(grouper(frame[\"pose_keypoints_2d\"], n_group))\n",
    "    else:\n",
    "        n_group = int(len(frame['keypoints']) / joints)\n",
    "        pose = list(grouper(frame[\"keypoints\"], n_group))\n",
    "    print(pose)\n",
    "    \n",
    "    if joints==25:\n",
    "        l_pair = l_pair_openpose25\n",
    "        line_color = colors_openpose25\n",
    "    elif joints == 26:\n",
    "        l_pair = l_pair_26\n",
    "        line_color = line_color_26\n",
    "        \n",
    "    line_cnt = 0\n",
    "    \n",
    "    for limb in l_pair:\n",
    "    #    print(limb)\n",
    "        if limb[0] == joints and (joints == 17 or joints == 26):\n",
    "            x1, y1, _ = (np.array(pose[5]) + np.array(pose[6])) / 2 #neck\n",
    "        else:\n",
    "            x1, y1, _ = pose[limb[0]]\n",
    "        x2, y2, _ = pose[limb[1]]\n",
    "        line = gz.polyline(points=[(x1,y1), (x2,y2)], stroke_width = 6, stroke=line_color[line_cnt])\n",
    "    #    print('line', line_cnt)\n",
    "        line_cnt += 1\n",
    "        line.draw(surface)\n",
    "    \n",
    "    for idx in range(len(pose)):\n",
    "    #    print(idx)\n",
    "        x1, y1,_ = pose[idx]\n",
    "        joint = gz.circle(4, xy=[x1,y1], fill=(0,0,0))\n",
    "        joint.draw(surface)\n",
    "\n",
    "    show_img(surface.get_npimage(), label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frame('D:\\Videos thesis\\SuJu16_poses\\SuJu16_poses\\\\frame7_keypoints.json', joints = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POST PROCESSING\n",
    "\n",
    "#check if difference between frames is too large\n",
    "\n",
    "\n",
    "#https://github.com/SonghaBan/DancingAI\n",
    "\n",
    "\n",
    "#def check_previous_frame(filename)\n",
    "\n",
    "#compute difference with one frame before here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sduxbury.medium.com/how-you-can-build-practical-applications-by-quantifying-observations-from-video-e266b945eea0\n",
    "#Point sets are in the form [coordinate, coordinate, confidence]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9368ae5186489c6741d2276028bf52b9e540a535f9e9252e679ab97e56563967"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
