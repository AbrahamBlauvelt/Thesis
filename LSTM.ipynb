{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter online\n"
     ]
    }
   ],
   "source": [
    "import gizeh as gz\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from more_itertools.recipes import grouper, pairwise\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Jupyter online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(jsonfile):\n",
    "    jsonopen = open(jsonfile)\n",
    "    jfile = json.load(jsonopen)\n",
    "    jsonopen.close()\n",
    "    x = pd.DataFrame(jfile)\n",
    "    return x\n",
    "\n",
    "def framenames(path, fps):\n",
    "    frames = np.arange(0, 900*fps+1, 24).tolist()\n",
    "    startname = 'frame'\n",
    "    endname = '_keypoints.json'\n",
    "    frames = frames[1:]\n",
    "    framelist = []\n",
    "    for i in frames:\n",
    "        framelist.append(path+startname+str(i)+endname)\n",
    "    return(framelist)\n",
    "\n",
    "def data_loader(framelist):\n",
    "    pose1= []\n",
    "    pose2= []\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "    for jsonfile in framelist:\n",
    "        data = open_json(jsonfile)\n",
    "        if len(data.index) == 2:\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[1,:]\n",
    "            count1 += 1\n",
    "        if len(data.index) == 3:\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[2,:]\n",
    "            count2 += 1\n",
    "        if len(data.index) == 1:\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[1,:]\n",
    "            count3 += 1\n",
    "        if len(data.index) == 4:        #NEEDS WORK, KN5 frame 216 has 4 skeletons in it\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[1,:]\n",
    "            count4 += 1\n",
    "        if len(data.index) == 5:\n",
    "            s1 = data.iloc[0,:]\n",
    "            s2 = data.iloc[1,:]\n",
    "            count5 += 1\n",
    "\n",
    "        sig1 = s1[\"people\"]\n",
    "        sig2 = s2[\"people\"]\n",
    "\n",
    "        signer1poses = pd.DataFrame.from_dict(sig1, orient='index')\n",
    "        signer2poses = pd.DataFrame.from_dict(sig2, orient='index')\n",
    "\n",
    "        signer1 = signer1poses.iloc[1:5,:]\n",
    "        signer2 = signer2poses.iloc[1:5,:]\n",
    "\n",
    "        signer1 = signer1.drop([\"face_keypoints_2d\"])\n",
    "        signer2 = signer2.drop([\"face_keypoints_2d\"])\n",
    "        \n",
    "        pose1.append(signer1)\n",
    "        pose2.append(signer2)\n",
    "    poses = pose1 + pose2\n",
    "    print('There are '+str(count3)+' frames with 1 person in them')\n",
    "    print('There are '+str(count1)+' frames with 2 persons in them')\n",
    "    print('There are '+str(count2)+' frames with 3 persons in them')\n",
    "    print('There are '+str(count4)+' frames with 4 persons in them')\n",
    "    print('There are '+str(count5)+' frames with 5 persons in them')\n",
    "    return poses\n",
    "\n",
    "def scale(lst):                                 #Look into standardscaler from sklearn\n",
    "    newdata = []\n",
    "    for i,d in enumerate(lst):\n",
    "        if d.any() > 0:\n",
    "           newdata.append(d/500)\n",
    "        else:\n",
    "            newdata.append(d)\n",
    "    return newdata\n",
    "\n",
    "#def scale(lst):\n",
    "#    scaler = StandardScaler()\n",
    "#    newdata = scaler.fit_transform(lst)\n",
    "#    return newdata\n",
    "\n",
    "def normalizer(lst,nmbr):\n",
    "    fin = []\n",
    "    norm = (nmbr*1.8)\n",
    "    #assuming person 2 always sits on the right in the frame which gives him/her different y coordinates but similar x coordinates.\n",
    "    z = np.array([norm,0])\n",
    "    #normalize the y coordinate appropriately for person 1 so relative distances stay the same. \n",
    "    for i,d in enumerate(lst):\n",
    "        if d.any() > 0:\n",
    "            d = d-z\n",
    "            fin.append(d)\n",
    "        else:\n",
    "            fin.append(d) \n",
    "    return fin\n",
    "\n",
    "def pose_processor(poses):                                                      #Finish pose processor\n",
    "    poses = poses\n",
    "    lengthp = len(poses)\n",
    "    middle = lengthp/2\n",
    "    p1 = poses[:int(middle)]\n",
    "    p2 = poses[int(middle):]\n",
    "\n",
    "    #Create confidenceintervals index\n",
    "    confidenceintervals = np.arange(-1, 75, 3).tolist()\n",
    "    confidenceintervals = confidenceintervals[1:]\n",
    "    #print(confidenceintervals)\n",
    "\n",
    "    #Create x and y coordinates index\n",
    "    coordinates = np.arange(0, 75, 1).tolist()\n",
    "    cs1 = set(confidenceintervals)\n",
    "    coorindex = [x for x in coordinates if x not in cs1]\n",
    "    #print(coorindex)\n",
    "\n",
    "    person1 = []\n",
    "    for framepose in p1:\n",
    "        signer1 = pd.DataFrame(framepose) \n",
    "        skel1 = signer1.iloc[0,:]\n",
    "        #Get confidence intervals for signer 1 and 2 per keypoint\n",
    "        ci1 = [skel1[i] for i in confidenceintervals]\n",
    "        #Get x and y coordinates for signer 1 and 2 per keypoint\n",
    "        skelcoor1 = [skel1[i] for i in coorindex]\n",
    "        sk1 = np.array_split(skelcoor1, 25)\n",
    "        #Normalize on neck Y coordinate as neck position\n",
    "        neck2 = sk1[1]\n",
    "        newdata1 = scale(normalizer(sk1, neck2[0]))\n",
    "        person1.append(newdata1)\n",
    "        \n",
    "    person2 = []\n",
    "    for framepose in p2:\n",
    "        signer2 = pd.DataFrame(framepose) \n",
    "        skel2 = signer2.iloc[0,:]\n",
    "        #Get confidence intervals for signer 1 and 2 per keypoint\n",
    "        ci2 = [skel2[i] for i in confidenceintervals]\n",
    "        #Get x and y coordinates for signer 1 and 2 per keypoint\n",
    "        skelcoor2 = [skel2[i] for i in coorindex]\n",
    "        sk2 = np.array_split(skelcoor2, 25)\n",
    "\n",
    "        newdata2 = scale(sk2)\n",
    "        person2.append(newdata2)\n",
    "    #return sk1, sk2\n",
    "    signers = person1 + person2\n",
    "    return signers\n",
    "\n",
    "def hand_processor(poses):                                          #Under  construction\n",
    "    poses = poses\n",
    "    lengthp = len(poses)\n",
    "    middle = lengthp/2\n",
    "    p1 = poses[:int(middle)]\n",
    "    p2 = poses[int(middle):]\n",
    "\n",
    "    #Create confidenceintervals index\n",
    "    confidenceintervalh = np.arange(-1, 63, 3).tolist()\n",
    "    confidenceintervalh = confidenceintervalh[1:]\n",
    "    #print(confidenceintervalh)\n",
    "\n",
    "    #Create x and y coordinates index\n",
    "    coordinateh = np.arange(0, 63, 1).tolist()\n",
    "    csi2 = set(confidenceintervalh)\n",
    "    coorindexhand = [x for x in coordinateh if x not in csi2]\n",
    "    #print(coorindexhand)\n",
    "\n",
    "    person1 = []                                        #CHANGE VARIABLE NAME\n",
    "    for framepose in p1:\n",
    "        signer1 = pd.DataFrame(framepose) \n",
    "        hand1_L = signer1.iloc[1,:]\n",
    "        hand1_R = signer1.iloc[2,:]\n",
    "        hand1_L = hand1_L.dropna(axis='rows')\n",
    "        hand1_R = hand1_R.dropna(axis='rows')\n",
    "        #Get confidence intervals for signer 1 and 2 per keypoint per hand\n",
    "        H1L = [hand1_L[i] for i in confidenceintervalh]\n",
    "        H1R = [hand1_R[i] for i in confidenceintervalh]\n",
    "        #Get x and y coordinates for signer 1 and 2 per keypoint\n",
    "        hand1R = [hand1_L[i] for i in coorindexhand]\n",
    "        hand1L = [hand1_R[i] for i in coorindexhand]\n",
    "        #print(len(hand1L))\n",
    "        #Split the list into x, y coordinate arrays\n",
    "        handR11 = np.array_split(hand1R, 21)\n",
    "        handL11 = np.array_split(hand1L, 21)\n",
    "        #Normalize on 240, from Songha Ban's code.\n",
    "        handR1 = scale(normalizer(handR11, 240))\n",
    "        handL1 = scale(normalizer(handL11, 240))\n",
    "        #Join them into 1 list with numpy arrays\n",
    "        hand1 = [handR1, handL1]\n",
    "        person1.append(hand1)\n",
    "\n",
    "    person2 = []                                        #CHANGE VARIABLE NAME\n",
    "    for framepose in p2:\n",
    "        signer2 = pd.DataFrame(framepose) \n",
    "        hand2_L = signer2.iloc[1,:]\n",
    "        hand2_R = signer2.iloc[2,:]\n",
    "        hand2_L = hand2_L.dropna(axis='rows')\n",
    "        hand2_R = hand2_R.dropna(axis='rows')\n",
    "        #Get confidence intervals for signer 1 and 2 per keypoint per hand\n",
    "        H2L = [hand2_R[i] for i in confidenceintervalh]\n",
    "        H2R = [hand2_R[i] for i in confidenceintervalh]\n",
    "        #Get x and y coordinates for signer 1 and 2 per keypoint\n",
    "        hand2R = [hand2_L[i] for i in coorindexhand]\n",
    "        hand2L = [hand2_R[i] for i in coorindexhand]\n",
    "        #print(len(hand1L))\n",
    "        #Split the list into x, y coordinate arrays\n",
    "        handR22 = np.array_split(hand2R, 21)\n",
    "        handL22 = np.array_split(hand2L, 21)\n",
    "        handR2 = scale(handR22)\n",
    "        handL2 = scale(handL22)\n",
    "        #Join them into 1 list with numpy arrays\n",
    "        hand2 = [handR2, handL2]\n",
    "        person2.append(hand2)\n",
    "    return person1, person2\n",
    "\n",
    "def data_shaper(signers, hands):\n",
    "    poses = signers\n",
    "    lengthp = len(poses)\n",
    "    middle = lengthp/2\n",
    "    p1 = poses[:int(middle)]\n",
    "    p2 = poses[int(middle):]\n",
    "    tot = []\n",
    "    for i in range(0,900):\n",
    "        newdata1 = []\n",
    "        for value in p1[i]:\n",
    "            for coordinate in value:\n",
    "                x = round(coordinate, 8)\n",
    "                newdata1.append(x)\n",
    "\n",
    "        newdata2 = []\n",
    "        for value in p2[i]:\n",
    "            for coordinate in value:\n",
    "                x = round(coordinate, 8)\n",
    "                newdata2.append(x)\n",
    "        shaped = np.array(newdata1 + newdata2)\n",
    "        tot.append(shaped)\n",
    "    end = np.array(tot)\n",
    "    end = end.reshape(1,900,100)\n",
    "    return end\n",
    "\n",
    "def label_loader(labelspath):\n",
    "    data = pd.read_csv(labelspath)\n",
    "    data2 = data.iloc[:,3]\n",
    "    labels = data2\n",
    "    labels = np.array(labels)\n",
    "    return labels\n",
    "\n",
    "def finish_processing(path,fps,labelspath):\n",
    "    framelist = framenames(path, fps)\n",
    "    poses = data_loader(framelist)\n",
    "    processedsk = pose_processor(poses)\n",
    "    #processedhands = hand_processor(poses)\n",
    "    features = processedsk\n",
    "    #features = data_shaper(processedsk,processedhands)\n",
    "    labels = label_loader(labelspath)\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 frames with 1 person in them\n",
      "There are 809 frames with 2 persons in them\n",
      "There are 90 frames with 3 persons in them\n",
      "There are 1 frames with 4 persons in them\n",
      "There are 0 frames with 5 persons in them\n",
      "(1800, 25, 2)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "path = 'D:\\Videos thesis\\KN5Jan7_poses\\KN5Jan7_poses\\\\'\n",
    "fps = 24\n",
    "labelspath = 'D:\\Videos thesis\\KN5jan7_annotated.csv'\n",
    "labels = label_loader(labelspath)\n",
    "z = finish_processing(path, fps, labelspath)\n",
    "print(z.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 100)\n",
      "(297, 100)\n",
      "(199, 100)\n",
      "(404,)\n",
      "(297,)\n",
      "(199,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "poses = z\n",
    "lengthp = len(poses)\n",
    "middle = lengthp/2\n",
    "p1 = poses[:int(middle)]\n",
    "p2 = poses[int(middle):]\n",
    "\n",
    "#explain what this does\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "tot1 = []\n",
    "tot2 = []\n",
    "for i in range(0,900):\n",
    "        newdata1 = []\n",
    "        for value in p1[i]:\n",
    "            for coordinate in value:\n",
    "                x = round(coordinate, 8)\n",
    "                newdata1.append(x)\n",
    "        newdata2 = []\n",
    "        for value in p2[i]:\n",
    "            for coordinate in value:\n",
    "                x = round(coordinate, 8)\n",
    "                newdata2.append(x)\n",
    "        tot1.append(newdata1)\n",
    "        tot2.append(newdata2)\n",
    "        #for value in labels[i]:\n",
    "        #        print(value)\n",
    "final1 = np.array(tot1)\n",
    "final2 = np.array(tot2)\n",
    "\n",
    "dataset = np.hstack((final1, final2))\n",
    "labels = pd.DataFrame(labels)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "#df = pd.DataFrame(dataset)\n",
    "#labels = pd.DataFrame(y)\n",
    "#mx = df.to_numpy().min()\n",
    "#print(mx)\n",
    "\n",
    "X = pd.DataFrame(dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.33, random_state = 1) # 0.26666 x 0.75 = 0.2\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "#X_train = X_train.reshape(1,len(X_train),100)\n",
    "print(X_train.shape)\n",
    "X_test = np.array(X_test)\n",
    "#X_test = X_test.reshape(1,len(X_test),100)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "print(X_val.shape)\n",
    "\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_train = np.squeeze(y_train)\n",
    "#y_train = y_train.reshape(1,len(y_train))\n",
    "print(y_train.shape)\n",
    "y_test = np.array(y_test)\n",
    "y_test = np.squeeze(y_test)\n",
    "#y_test = y_test.reshape(1,len(y_test))\n",
    "print(y_test.shape)\n",
    "\n",
    "y_val = np.array(y_val)\n",
    "y_val = np.squeeze(y_val)\n",
    "print(y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(recurrent_units,\n",
    "               batch_size,\n",
    "               epochs,\n",
    "               dropout_rate,\n",
    "               optimizer,\n",
    "               learning_rate,\n",
    "               n_timesteps,\n",
    "               n_features,\n",
    "               n_outputs):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(recurrent_units, input_shape=(n_timesteps,n_features), recurrent_dropout=dropout_rate))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer(lr=learning_rate), metrics=['accuracy', metrics.Precision(), metrics.Recall(), metrics.AUC()])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 100)\n",
      "(1, 297, 100)\n",
      "(1, 603, 100)\n",
      "(1, 603)\n",
      "(1, 297)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h\\AppData\\Local\\Temp/ipykernel_22316/3389187666.py:48: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_pipeline = Pipeline([(\"clf\", KerasClassifier(build_fn=make_model))])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22316/3389187666.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m                                       scoring=['f1_macro', 'roc_auc', 'accuracy'])\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mrs_keras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best Params:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs_keras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\h\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\h\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1766\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1767\u001b[0m             ParameterSampler(\n\u001b[0;32m   1768\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\h\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    848\u001b[0m                         \u001b[1;33m**\u001b[0m\u001b[0mfit_and_score_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m                     )\n\u001b[1;32m--> 850\u001b[1;33m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0m\u001b[0;32m    851\u001b[0m                         \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m                     )\n",
      "\u001b[1;32mc:\\Users\\h\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    334\u001b[0m                 (\n\u001b[0;32m    335\u001b[0m                     \u001b[1;34m\"Cannot have number of splits n_splits={0} greater\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=1."
     ]
    }
   ],
   "source": [
    "verbose = 1\n",
    "fold = 1\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "\n",
    "X = np.array(dataset)\n",
    "\n",
    "y = np.array(labels)\n",
    "y = y.squeeze()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "\n",
    "n_timesteps, n_features, n_outputs = X.shape[0], X.shape[1], 1\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.333333, random_state = 1)\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test = X_test.reshape(1,len(X_test),100)\n",
    "print(X_test.shape)\n",
    "X_train = X_train.reshape(1,len(X_train),100)\n",
    "print(X_train.shape)\n",
    "\n",
    "y_train = y_train.reshape(1,len(y_train))\n",
    "print(y_train.shape)\n",
    "y_test = y_test.reshape(1,len(y_test))\n",
    "print(y_test.shape)\n",
    "\n",
    "        #print(X_test.shape)\n",
    "        #X_test = X_test.reshape(5,int(len(X_test)/5),100)\n",
    "        #print(X_test.shape)\n",
    "        #X_train = X_train.reshape(5,int(len(X_train)/5),100)\n",
    "        #print(X_train.shape)\n",
    "        #X_val = X_val.reshape(5,int(len(X_val)/5),100)\n",
    "        #print(X_val.shape)\n",
    "\n",
    "        #y_train = y_train.reshape(5,int(len(y_train)/5))\n",
    "        #print(y_train.shape)\n",
    "        #y_test = y_test.reshape(5,int(len(y_test)/5))\n",
    "        #print(y_test.shape)\n",
    "        #y_val = y_val.reshape(5,int(len(y_val)/5))\n",
    "        #print(y_val.shape)\n",
    "\n",
    "\n",
    "keras_pipeline = Pipeline([(\"clf\", KerasClassifier(build_fn=make_model))])\n",
    "\n",
    "param_grid = {\n",
    "            'clf__recurrent_units': [25, 50, 75, 100, 125],\n",
    "            'clf__batch_size': [1, 5, 10, 15, 35],\n",
    "            'clf__epochs': [10, 20, 50, 100, 200],\n",
    "            'clf__dropout_rate': [0.0, 0.05, 0.1, 0.25, 0.5],\n",
    "            'clf__optimizer': [optimizers.Adam, optimizers.Nadam, optimizers.RMSprop],\n",
    "            'clf__learning_rate': [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "            'clf__class_weight': [{0: 1}],\n",
    "            'clf__n_timesteps': [n_timesteps],\n",
    "            'clf__n_features': [n_features],\n",
    "            'clf__n_outputs': [n_outputs],\n",
    "            'clf__verbose': [verbose],\n",
    "}\n",
    "\n",
    "rs_keras = RandomizedSearchCV(keras_pipeline,\n",
    "                                      param_distributions=param_grid,\n",
    "                                      refit='accuracy',\n",
    "                                      n_iter=10,\n",
    "                                      n_jobs=1,\n",
    "                                      scoring=['f1_macro', 'roc_auc', 'accuracy'])\n",
    "\n",
    "rs_keras.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params:', rs_keras.best_params_)\n",
    "\n",
    "model = make_model(\n",
    "            recurrent_units=rs_keras.best_params_['clf__recurrent_units'],\n",
    "            batch_size=rs_keras.best_params_['clf__batch_size'],\n",
    "            epochs=rs_keras.best_params_['clf__epochs'],\n",
    "            dropout_rate=rs_keras.best_params_['clf__dropout_rate'],\n",
    "            optimizer=rs_keras.best_params_['clf__optimizer'],\n",
    "            learning_rate=rs_keras.best_params_['clf__learning_rate'],\n",
    "            n_timesteps=n_timesteps,\n",
    "            n_features=n_features,\n",
    "            n_outputs=n_outputs\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                  batch_size=rs_keras.best_params_['clf__batch_size'],\n",
    "                  epochs=rs_keras.best_params_['clf__epochs'],\n",
    "                  verbose=verbose,\n",
    "                  validation_data=(X_train, y_train),\n",
    "                  class_weight={0: 1}\n",
    ")\n",
    "\n",
    "#                  validation_data=[(X_train, y_train), (X_val, y_val)],\n",
    "\n",
    "show_training_plot(history)\n",
    "\n",
    "    \n",
    "y_pred = model.predict(X[test])\n",
    "show_roc_plot(y[test], y_pred)\n",
    "\n",
    "    #y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y[test], y_pred_classes))\n",
    "f1 = f1_score(y[test], y_pred_classes, average='macro')\n",
    "print('F1-score:', f1)\n",
    "f1_scores.append(f1)\n",
    "roc_auc = roc_auc_score(y[test], y_pred_classes)\n",
    "print('AUC score:', roc_auc)\n",
    "roc_auc_scores.append(roc_auc)\n",
    "accuracy = accuracy_score(y[test], y_pred_classes)\n",
    "print('Accuracy score:', accuracy)\n",
    "accuracy_scores.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 100)\n",
      "Fold: # 1\n",
      "(450, 100)\n",
      "(1, 450, 1, 100)\n",
      "(1, 450, 100)\n",
      "(1, 450)\n",
      "(1, 450)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h\\AppData\\Local\\Temp/ipykernel_22316/220830071.py:76: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_pipeline = Pipeline([(\"clf\", KerasClassifier(build_fn=make_model))])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22316/220830071.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m                                       scoring=['f1_macro', 'roc_auc', 'accuracy'])\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mrs_keras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best Params:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs_keras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\h\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\h\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1766\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1767\u001b[0m             ParameterSampler(\n\u001b[0;32m   1768\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\h\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    848\u001b[0m                         \u001b[1;33m**\u001b[0m\u001b[0mfit_and_score_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m                     )\n\u001b[1;32m--> 850\u001b[1;33m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0m\u001b[0;32m    851\u001b[0m                         \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m                     )\n",
      "\u001b[1;32mc:\\Users\\h\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    334\u001b[0m                 (\n\u001b[0;32m    335\u001b[0m                     \u001b[1;34m\"Cannot have number of splits n_splits={0} greater\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=1."
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import optimizers, metrics, regularizers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "\n",
    "X = np.array(dataset)\n",
    "\n",
    "print(X.shape)\n",
    "y = np.array(labels)\n",
    "y = y.squeeze()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "n_timesteps, n_features, n_outputs = int(X.shape[0]/5), X.shape[1], 1\n",
    "\n",
    "kf = StratifiedKFold(n_splits=2, random_state=420, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "#originally from https://github.com/aryaprabawa/Startup-Invest-Prediction/blob/master/Predict_Investment_With_AUs.ipynb \n",
    "verbose = 0\n",
    "fold = 1\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "accuracy_scores = []\n",
    "for train, test in kf.split(X, y):\n",
    "        print('Fold: #', fold)\n",
    "        y_train = y[train]\n",
    "        X_train = X[train]\n",
    "        y_test = y[test]\n",
    "        X_test = X[test]\n",
    "\n",
    "        n_timesteps, n_features, n_outputs = X.shape[0], X.shape[1], 1\n",
    "        #X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.333333, random_state = 1)\n",
    "\n",
    "        \n",
    "        print(X_test.shape)\n",
    "        X_test = X_test.reshape(1,len(X_test),1,100)\n",
    "        print(X_test.shape)\n",
    "        X_train = X_train.reshape(1,len(X_train),100)\n",
    "        print(X_train.shape)\n",
    "\n",
    "        y_train = y_train.reshape(1,len(y_train))\n",
    "        print(y_train.shape)\n",
    "        y_test = y_test.reshape(1,len(y_test))\n",
    "        print(y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print(X_test.shape)\n",
    "        #X_test = X_test.reshape(5,int(len(X_test)/5),100)\n",
    "        #print(X_test.shape)\n",
    "        #X_train = X_train.reshape(5,int(len(X_train)/5),100)\n",
    "        #print(X_train.shape)\n",
    "        #X_val = X_val.reshape(5,int(len(X_val)/5),100)\n",
    "        #print(X_val.shape)\n",
    "\n",
    "        #y_train = y_train.reshape(5,int(len(y_train)/5))\n",
    "        #print(y_train.shape)\n",
    "        #y_test = y_test.reshape(5,int(len(y_test)/5))\n",
    "        #print(y_test.shape)\n",
    "        #y_val = y_val.reshape(5,int(len(y_val)/5))\n",
    "        #print(y_val.shape)\n",
    "\n",
    "\n",
    "        keras_pipeline = Pipeline([(\"clf\", KerasClassifier(build_fn=make_model))])\n",
    "\n",
    "        param_grid = {\n",
    "            'clf__recurrent_units': [25, 50, 75, 100, 125],\n",
    "            'clf__batch_size': [1, 5, 10, 15, 35],\n",
    "            'clf__epochs': [10, 20, 50, 100, 200],\n",
    "            'clf__dropout_rate': [0.0, 0.05, 0.1, 0.25, 0.5],\n",
    "            'clf__optimizer': [optimizers.Adam, optimizers.Nadam, optimizers.RMSprop],\n",
    "            'clf__learning_rate': [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "            'clf__class_weight': [{0: 1}],\n",
    "            'clf__n_timesteps': [n_timesteps],\n",
    "            'clf__n_features': [n_features],\n",
    "            'clf__n_outputs': [n_outputs],\n",
    "            'clf__verbose': [verbose],\n",
    "        }\n",
    "\n",
    "        rs_keras = RandomizedSearchCV(keras_pipeline,\n",
    "                                      param_distributions=param_grid,\n",
    "                                      refit='accuracy',\n",
    "                                      n_iter=10,\n",
    "                                      n_jobs=1,\n",
    "                                      scoring=['f1_macro', 'roc_auc', 'accuracy'])\n",
    "\n",
    "        rs_keras.fit(X_train, y_train)\n",
    "\n",
    "        print('Best Params:', rs_keras.best_params_)\n",
    "\n",
    "        model = make_model(\n",
    "            recurrent_units=rs_keras.best_params_['clf__recurrent_units'],\n",
    "            batch_size=rs_keras.best_params_['clf__batch_size'],\n",
    "            epochs=rs_keras.best_params_['clf__epochs'],\n",
    "            dropout_rate=rs_keras.best_params_['clf__dropout_rate'],\n",
    "            optimizer=rs_keras.best_params_['clf__optimizer'],\n",
    "            learning_rate=rs_keras.best_params_['clf__learning_rate'],\n",
    "            n_timesteps=n_timesteps,\n",
    "            n_features=n_features,\n",
    "            n_outputs=n_outputs\n",
    "        )\n",
    "\n",
    "        history = model.fit(X_train, y_train,\n",
    "                  batch_size=rs_keras.best_params_['clf__batch_size'],\n",
    "                  epochs=rs_keras.best_params_['clf__epochs'],\n",
    "                  verbose=verbose,\n",
    "                  validation_data=(X_train, y_train),\n",
    "                  class_weight={0: 1}\n",
    "        )\n",
    "\n",
    "#                  validation_data=[(X_train, y_train), (X_val, y_val)],\n",
    "\n",
    "        show_training_plot(history)\n",
    "\n",
    "    \n",
    "        y_pred = model.predict(X[test])\n",
    "        show_roc_plot(y[test], y_pred)\n",
    "\n",
    "    #y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "        print(classification_report(y[test], y_pred_classes))\n",
    "        f1 = f1_score(y[test], y_pred_classes, average='macro')\n",
    "        print('F1-score:', f1)\n",
    "        f1_scores.append(f1)\n",
    "        roc_auc = roc_auc_score(y[test], y_pred_classes)\n",
    "        print('AUC score:', roc_auc)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        accuracy = accuracy_score(y[test], y_pred_classes)\n",
    "        print('Accuracy score:', accuracy)\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "        fold = fold + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15632/377207332.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# Pad all sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mpadded_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 0.0 because it corresponds with <PAD>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mpadded_inputs_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 0.0 because it corresponds with <PAD>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    150\u001b[0m           \u001b[1;32mor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m   \"\"\"\n\u001b[1;32m--> 152\u001b[1;33m   return sequence.pad_sequences(\n\u001b[0m\u001b[0;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m       padding=padding, truncating=truncating, value=value)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pre'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Padding type \"%s\" not understood'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FORMAT FOR LSTM INPUT\n",
    "# [samples, time steps and features]\n",
    "# [number of examples, 1 fps and 50 features per timestep (fps)]\n",
    "np.random.seed(0)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "##                          ##\n",
    "##        MODEL             ##\n",
    "##                          ##\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "additional_metrics = ['accuracy']\n",
    "batch_size = 128\n",
    "embedding_output_dims = 15\n",
    "loss_function = BinaryCrossentropy()\n",
    "max_sequence_length = 300\n",
    "num_distinct_words = 5000\n",
    "number_of_epochs = 5\n",
    "optimizer = Adam()\n",
    "validation_split = 0.20\n",
    "verbosity_mode = 1\n",
    "\n",
    "# Disable eager execution\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_distinct_words)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# Pad all sequences\n",
    "padded_inputs = pad_sequences(x_train, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "\n",
    "# Define the Keras model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_distinct_words, embedding_output_dims, input_length=max_sequence_length))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n",
    "\n",
    "# Give a summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_inputs, y_train, batch_size=batch_size, epochs=number_of_epochs, verbose=verbosity_mode, validation_split=validation_split)\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spyjetson.blogspot.com/2019/10/jetsonnano-human-pose-estimation-using.html\n",
    "\n",
    "#SKELETON KEYPOINTS\n",
    "{\n",
    "//     {0,  \"Nose\"},\n",
    "//     {1,  \"Neck\"},\n",
    "//     {2,  \"RShoulder\"},\n",
    "//     {3,  \"RElbow\"},\n",
    "//     {4,  \"RWrist\"},\n",
    "//     {5,  \"LShoulder\"},\n",
    "//     {6,  \"LElbow\"},\n",
    "//     {7,  \"LWrist\"},\n",
    "//     {8,  \"MidHip\"},\n",
    "//     {9,  \"RHip\"},\n",
    "//     {10, \"RKnee\"},\n",
    "//     {11, \"RAnkle\"},\n",
    "//     {12, \"LHip\"},\n",
    "//     {13, \"LKnee\"},\n",
    "//     {14, \"LAnkle\"},\n",
    "//     {15, \"REye\"},\n",
    "//     {16, \"LEye\"},\n",
    "//     {17, \"REar\"},\n",
    "//     {18, \"LEar\"},\n",
    "//     {19, \"LBigToe\"},\n",
    "//     {20, \"LSmallToe\"},\n",
    "//     {21, \"LHeel\"},\n",
    "//     {22, \"RBigToe\"},\n",
    "//     {23, \"RSmallToe\"},\n",
    "//     {24, \"RHeel\"},\n",
    "// };\n",
    "\n",
    "#HAND KEYPOINTS\n",
    "#as seen on left hand when it is held open \n",
    "{\n",
    "//     {0,  \"BotR palm\"},\n",
    "//     {1,  \"BotL palm\"},\n",
    "//     {2,  \"Thumb Bot\"},\n",
    "//     {3,  \"Thumb Mid\"},\n",
    "//     {4,  \"Thumb Top\"},\n",
    "//     {5,  \"Index Bot\"},\n",
    "//     {6,  \"Index Mid1\"},\n",
    "//     {7,  \"Index Mid2\"},\n",
    "//     {8,  \"Index Top\"},\n",
    "//     {9,  \"Middle Bot\"},\n",
    "//     {10, \"Middle Mid1\"},\n",
    "//     {11, \"Middle Mid2\"},\n",
    "//     {12, \"Middle Top\"},\n",
    "//     {13, \"Ring Bot\"},\n",
    "//     {14, \"Ring Mid1\"},\n",
    "//     {15, \"Ring Mid2\"},\n",
    "//     {16, \"Ring Top\"},\n",
    "//     {17, \"Pinky Bot\"},\n",
    "//     {18, \"Pinky Mid1\"},\n",
    "//     {19, \"Pinky Mid2\"},\n",
    "//     {20, \"Pinky Top\"},\n",
    "// };\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "012e77c2d74e6bca337c7e0b81e2a9542c19e12b2d604af55c024b669df346a3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
